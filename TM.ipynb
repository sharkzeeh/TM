{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bqVHAgCW_SCX"
   },
   "source": [
    "# Step 1: Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6789,
     "status": "ok",
     "timestamp": 1589458733545,
     "user": {
      "displayName": "Igor Babikov",
      "photoUrl": "",
      "userId": "15520234525783945183"
     },
     "user_tz": -180
    },
    "id": "K2-oEo1s_SCa",
    "outputId": "28a30834-ca02-4229-f237-8b658a7f0447"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pprint\n",
    "import artm\n",
    "from pymystem3 import Mystem\n",
    "import string\n",
    "import functools\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 20349,
     "status": "ok",
     "timestamp": 1589458747188,
     "user": {
      "displayName": "Igor Babikov",
      "photoUrl": "",
      "userId": "15520234525783945183"
     },
     "user_tz": -180
    },
    "id": "dP9y_RkJ_SCp",
    "outputId": "8bf31b35-cbcd-49da-b070-6fd13f8c0c66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.93 s, sys: 1.15 s, total: 6.07 s\n",
      "Wall time: 11.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "DATA = 'data/ydistricts_new_data.json'\n",
    "with open(DATA) as fh:\n",
    "    posts = json.load(fh)\n",
    "    \n",
    "raw_sents = [sent for sent in list(posts['text'].values()) if sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/robez/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils.data_process import process_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82
    },
    "colab_type": "code",
    "id": "gW7OkrKa_SC9",
    "outputId": "5757360d-5888-4c1a-b4f0-14876704ace0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 196326/196326 [02:35<00:00, 1260.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing short words...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# the cell takes 2-3 minutes to run..\n",
    "processed = process_text(raw_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2UP4SWNB_SDG"
   },
   "source": [
    "### Pickling preprocessed texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-XH1mXup_SDI"
   },
   "outputs": [],
   "source": [
    "def get_proccessed_sents_from_pickle(path=\"processed.pickle\", raw_sents=None):\n",
    "    if not os.path.exists(path) or not os.stat(path).st_size:\n",
    "        with open(path, 'wb') as fh:\n",
    "            processed = process_text(raw_sents)\n",
    "            pickle.dump(processed, fh, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(path, 'rb') as fh:\n",
    "        processed = pickle.load(fh)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "peyvK9aP_SDN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'маленький собачка замерзать припарковывать машина омский улица'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed = get_proccessed_sents_from_pickle()\n",
    "processed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6B6VCYOs_SDV"
   },
   "source": [
    "# Step 2: Batches preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_path = \"wabbit_on_cleaned_texts.txt\"\n",
    "\n",
    "def check_path(fn=None, *, PATH=vocab_path):\n",
    "    \n",
    "    if fn is None:\n",
    "        return lambda fn: check_path(fn, PATH=PATH)\n",
    "\n",
    "    @functools.wraps(fn)\n",
    "    def func(*args, **kwargs):\n",
    "        if not os.path.exists(PATH) or not os.stat(PATH).st_size:\n",
    "            fn(*args, **kwargs)\n",
    "        else:\n",
    "            print(f'{PATH} already exists!')\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PIbv1nVl_SDb"
   },
   "outputs": [],
   "source": [
    "@check_path\n",
    "def vocabulary_prep(text, vocab):\n",
    "     with open(vocab, 'w') as fh:\n",
    "        for sent in text:\n",
    "            fh.write(' |text ' + sent + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_prep(processed, vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ogjv6xf3_SDy"
   },
   "source": [
    "##### Prepare batches with artm.BatchVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kfQ8Q-oT_SD1"
   },
   "outputs": [],
   "source": [
    "def batching(batch_path=\"batches/cleaned_texts\", data_path=vocab_path, text_df=\"vowpal_wabbit\", batches_df=\"batches\", batch_size=1024):\n",
    "    \n",
    "    if not os.path.exists(batch_path):\n",
    "        os.makedirs(batch_path)\n",
    "\n",
    "    if not os.listdir(path=batch_path):\n",
    "        batch_vectorizer = artm.BatchVectorizer(\n",
    "                                            data_path=data_path,\n",
    "                                            data_format=text_df, \n",
    "                                            target_folder=batch_path, \n",
    "                                            batch_size=batch_size)\n",
    "    else:\n",
    "        batch_vectorizer = artm.BatchVectorizer(data_path=batch_path,\n",
    "                                            data_format=batches_df)\n",
    "        \n",
    "    return batch_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hQ0bA-T9_SD5"
   },
   "outputs": [],
   "source": [
    "batch_vectorizer = batching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dictionary\n",
    "dictionary = artm.Dictionary(name='dictionary')\n",
    "dictionary.gather(batch_vectorizer.data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_vectorizer.dictionary.save_text('vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#! /bin/bash\r\n",
      "\r\n",
      "bigartm -c vw.txt -v vocab.txt --cooc-window 10 --cooc-min-tf 5 --write-cooc-tf cooc_tf_ --cooc-min-df 5 --write-cooc-df cooc_df_ --write-ppmi-tf ppmi_tf_ --write-ppmi-df ppmi_df_\r\n"
     ]
    }
   ],
   "source": [
    "!cat bigartm-book/junk/cooc_dictionary/launch.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cat bigartm-book/junk/cooc_dictionary/vw.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cd bigartm-book/junk/cooc_dictionary/\n",
    "# !bash bigartm -c ../../../wabbit_on_cleaned_texts.txt -v vocab.txt --cooc-window 10 --cooc-min-tf 5 --write-cooc-tf cooc_tf_ --cooc-min-df 5 --write-cooc-df cooc_df_ --write-ppmi-tf ppmi_tf_ --write-ppmi-df ppmi_df_\n",
    "# # Что данные ключи значат, по порядку:\n",
    "# !cd -\n",
    "# # -c − прочитать коллекцию;\n",
    "# # -v − прочитать vocab;\n",
    "# # --cooc-window − задать ширину окна (со-встречаемость токенов учитывается только внутри некоторого окна);\n",
    "# # --cooc-min-tf − задать минимальное значение cooc TF (значение ниже данного порога не будут писаться в выходной файл);\n",
    "# # --cooc-min-df − аналогично предыдущему, только для cooc DF;\n",
    "# # --write-cooc-tf − записать файл с cooc TF по заданному пути, далее указывается путь;\n",
    "# # --write-cooc-df, --write-ppmi-tf, --write-ppmi-df − аналогично для файлов cooc DF, PPMI TF, PPMI DF."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T59-ebcG_SEB"
   },
   "source": [
    "# Step 3: Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RfLHy_tq_SEV"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "TOPICS = [5, 10, 25, 40]\n",
    "TOKENS = 50\n",
    "PASSES = [1, 3]\n",
    "sparse_tau = [-5e-2, -1e-1, -1e1, -1e2, -1e3]\n",
    "    \n",
    "hyperparameters = product(TOPICS, PASSES)\n",
    "\n",
    "evals = [\n",
    "    artm.PerplexityScore(name='PerplexityScore', dictionary='dictionary'),\n",
    "    artm.SparsityPhiScore(name='SparsityPhiScore', class_id=\"text\"),\n",
    "    artm.SparsityThetaScore(name='SparsityThetaScore')]\n",
    "\n",
    "def scores_adder(model, evals=evals):\n",
    "    for sc in evals:\n",
    "        model.scores.add(sc, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import product\n",
    "\n",
    "# TOPICS = [10, 40]\n",
    "# TOKENS = 50\n",
    "# PASSES = [1]\n",
    "# sparse_tau = [-5e-2, -1e1]\n",
    "    \n",
    "# hyperparameters = product(TOPICS, PASSES)\n",
    "\n",
    "# evals = [\n",
    "#     artm.PerplexityScore(name='PerplexityScore', dictionary='dictionary'),\n",
    "#     artm.SparsityPhiScore(name='SparsityPhiScore', class_id=\"text\"),\n",
    "#     artm.SparsityThetaScore(name='SparsityThetaScore')]\n",
    "\n",
    "# def scores_adder(model, evals=evals):\n",
    "#     for sc in evals:\n",
    "#         model.scores.add(sc, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://nbviewer.jupyter.org/github/bigartm/bigartm-book/blob/master/junk/cooc_dictionary/example_of_gathering.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVeexSyh_SEl"
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "models_reg = []\n",
    "\n",
    "def train(hypes=None, num_collection_passes=15):\n",
    "    \n",
    "    if not os.path.exists('models'):\n",
    "        os.mkdir('models')\n",
    "    \n",
    "    best_phi_sparsity, best_theta_sparsity, best_model = None, None, None\n",
    "    best_phi_sparsity_reg, best_theta_sparsity_reg, best_model_reg = None, None, None\n",
    "    \n",
    "    for top, passes in hypes:\n",
    "\n",
    "        print(f\"Params: # topics = {top}; # single doc passes = {passes}\")\n",
    "\n",
    "        # initializing the ARTM model\n",
    "        model = artm.ARTM(\n",
    "                num_topics=top, \n",
    "                topic_names=[\"topic\"+str(i) for i in range(top)],\n",
    "                class_ids={\"text\":1},\n",
    "                reuse_theta=True, \n",
    "                cache_theta=True,\n",
    "                num_document_passes=passes,\n",
    "                seed=42)\n",
    "\n",
    "        # adding scores\n",
    "        scores_adder(model, evals)\n",
    "        model.scores.add(artm.TopTokensScore(name=\"top_words\", num_tokens=TOKENS, class_id=\"text\"),\n",
    "                        overwrite=True)\n",
    "\n",
    "        # initialize the dictionary `dictionary` on the model `model`\n",
    "        model.initialize('dictionary')\n",
    "\n",
    "        # training\n",
    "        model.fit_offline(batch_vectorizer=batch_vectorizer,\n",
    "                          num_collection_passes=num_collection_passes)\n",
    "\n",
    "        # choosing the best model by the following criteria\n",
    "        cur_phi = model.score_tracker[\"SparsityPhiScore\"].last_value\n",
    "        cur_theta = model.score_tracker[\"SparsityThetaScore\"].last_value\n",
    "\n",
    "        print(round(cur_phi, 3), round(cur_theta, 3))\n",
    "        model.dump_artm_model(f'models/model_{top}_{passes}')\n",
    "        \n",
    "        if (best_phi_sparsity is None and best_theta_sparsity is None) or \\\n",
    "            (cur_phi >= best_phi_sparsity and cur_theta >= best_theta_sparsity):\n",
    "\n",
    "            best_phi_sparsity = cur_phi\n",
    "            best_theta_sparsity = cur_theta\n",
    "            best_model = model\n",
    "            \n",
    "        #####################################################################################################\n",
    "\n",
    "        print(\"\\nUsing regularizers...\")\n",
    "        model.regularizers.add(artm.SmoothSparsePhiRegularizer(name='SparsePhi', tau=0, dictionary=dictionary),\n",
    "                              overwrite=True)\n",
    "        model.regularizers.add(artm.SmoothSparseThetaRegularizer(name='SparseTheta', tau=0),\n",
    "                              overwrite=True)\n",
    "\n",
    "        for st in sparse_tau:\n",
    "            print('- - - - - - - - - - - -')\n",
    "            print(f\"Tau of sparse phi/theta {st}\")\n",
    "            model.regularizers['SparsePhi'].tau = st\n",
    "            model.regularizers[\"SparseTheta\"].tau = st\n",
    "            assert(model.regularizers['SparsePhi'].tau == st)\n",
    "\n",
    "            model.fit_offline(batch_vectorizer=batch_vectorizer,\n",
    "                              num_collection_passes=num_collection_passes)\n",
    "            \n",
    "            cur_phi_reg = model.score_tracker[\"SparsityPhiScore\"].last_value\n",
    "            cur_theta_reg = model.score_tracker[\"SparsityThetaScore\"].last_value\n",
    "\n",
    "            print(round(cur_phi_reg, 3), round(cur_theta_reg, 3))\n",
    "\n",
    "            # make sure the matrices do not get nulled\n",
    "            if cur_phi_reg == 1.0 or cur_theta_reg == 1.0:\n",
    "                continue\n",
    "            model.dump_artm_model(f'models/reg_model_{top}_{passes}_{st}')\n",
    "\n",
    "            if (best_phi_sparsity_reg is None and best_theta_sparsity_reg is None) or \\\n",
    "                (cur_phi_reg >= best_phi_sparsity and cur_theta_reg >= best_theta_sparsity_reg):\n",
    "\n",
    "                best_phi_sparsity_reg = cur_phi_reg\n",
    "                best_theta_sparsity_reg = cur_theta_reg\n",
    "                best_model_reg = model\n",
    "            \n",
    "        print('===========================================================================')\n",
    "        \n",
    "    return best_model, best_model_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: # topics = 10; # single doc passes = 1\n",
      "0.223 0.0\n",
      "\n",
      "Using regularizers...\n",
      "- - - - - - - - - - - -\n",
      "Tau of sparse phi/theta -0.05\n",
      "0.571 0.695\n",
      "- - - - - - - - - - - -\n",
      "Tau of sparse phi/theta -10.0\n",
      "0.723 0.958\n",
      "===========================================================================\n",
      "Params: # topics = 40; # single doc passes = 1\n",
      "0.433 0.001\n",
      "\n",
      "Using regularizers...\n",
      "- - - - - - - - - - - -\n",
      "Tau of sparse phi/theta -0.05\n",
      "0.788 0.916\n",
      "- - - - - - - - - - - -\n",
      "Tau of sparse phi/theta -10.0\n",
      "0.895 0.991\n",
      "===========================================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# train and save every single model\n",
    "\n",
    "%%time\n",
    "train(hypes=hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SuPU9XPr_SDq"
   },
   "source": [
    "#############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_printer(model=None, tracker=\"top_words\"):\n",
    "    s = ''\n",
    "    tokens = model.score_tracker[tracker].tokens[0]\n",
    "#     print(model.score_tracker[tracker].last_tokens) #  empty dict\n",
    "    for topic_name in model.topic_names:\n",
    "        try:\n",
    "            s += topic_name +':' + \",\".join(tokens[topic_name]) + \"\\n\"\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_topics(model=None, tracker=\"top_words\", file=\"topics.txt\"):\n",
    "    with open(file, 'w') as fh:\n",
    "        fh.write(tokens_printer(model, tracker))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_names = os.listdir('models') # list of all models (non-reg and reg)\n",
    "all_models = []\n",
    "if not os.path.exists('topics'):\n",
    "    os.mkdir('topics')\n",
    "\n",
    "for model_name in all_model_names:\n",
    "    \n",
    "    model = artm.load_artm_model(f'models/{model_name}')\n",
    "    all_models.append(model)\n",
    "    \n",
    "    n_doc_passes = model.num_document_passes\n",
    "\n",
    "    if not model_name.startswith('reg'):    \n",
    "        topics_filename = f'topics/topics_{model.num_topics}_{n_doc_passes}passes.txt'\n",
    "\n",
    "    else:\n",
    "        phi = model.regularizers['SparsePhi'].tau\n",
    "        theta = model.regularizers['SparseTheta'].tau\n",
    "\n",
    "        topics_filename = f'topics/topics_{model.num_topics}_{n_doc_passes}passes_{phi}phi_{theta}theta.txt'\n",
    "    save_topics(model, file=topics_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J97UzJGc_SFP"
   },
   "source": [
    "#### инфер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "niEGvsXM_SFb"
   },
   "outputs": [],
   "source": [
    "def infer(model=None, n_examples=25):\n",
    "    r = np.random.randint(0, high=len(predict_t), size=n_examples)\n",
    "    # get the most probable topic for a processed text\n",
    "    # https://thispointer.com/pandas-find-maximum-values-position-in-columns-or-rows-of-a-dataframe/\n",
    "    samples = dict(predict_t.iloc[r].idxmax(axis=1))\n",
    "#     tokens = model.score_tracker[\"top_words\"].last_tokens\n",
    "    tokens = model.score_tracker['top_words'].tokens[0]\n",
    "    for k, v in samples.items():\n",
    "        s = ''\n",
    "        try:\n",
    "            n_topics = model.num_topics\n",
    "            n_doc_passes = model.num_document_passes\n",
    "            phi_sparse = model.score_tracker[\"SparsityPhiScore\"].last_value\n",
    "            theta_sparse = model.score_tracker[\"SparsityThetaScore\"].last_value\n",
    "            \n",
    "            phi, theta = None, None\n",
    "            if model.regularizers:\n",
    "                phi = model.regularizers['SparsePhi'].tau\n",
    "                theta = model.regularizers['SparseTheta'].tau\n",
    "            \n",
    "            s += f'Модель: {n_topics} тем; {n_doc_passes} проходов по док-у'\n",
    "            try:\n",
    "                s += f'; {phi:.2f} phi reg; {theta:.2f} theta reg\\n'\n",
    "            except:\n",
    "                s += '\\n'\n",
    "                \n",
    "            s += f'SparsePhi: {phi_sparse:.3f}; SparseTheta: {theta_sparse:.3f}\\n'\n",
    "            s += f'Текст: {processed[k]}\\nТокены по теме: {tokens[v]}\\n\\n'\n",
    "            return s\n",
    "        except KeyError:\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "niEGvsXM_SFb"
   },
   "outputs": [],
   "source": [
    "with open('examples.txt', 'w') as fh:\n",
    "    for model in all_models:\n",
    "        predict = model.transform(batch_vectorizer=batch_vectorizer)\n",
    "        predict_t = predict.T # theta matrix\n",
    "        fh.write(infer(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "niEGvsXM_SFb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель: 10 тем; 1 проходов по док-у\r\n",
      "Текст: утерять документ паспорт водительский удостоверение имя самохин татьяна юрьевна находить просить вернуть вознаграждение написать лс спасибо\r\n",
      "Токены по теме: ['это', 'сосед', 'год', 'человек', 'подсказывать', 'день', 'который', 'район', 'город', 'знать', 'наш', 'свой', 'хотеть', 'ребенок', 'очень', 'новый', 'весь', 'собака', 'мочь', 'привет', 'дом', 'хороший', 'пожалуйста', 'улица', 'время', 'работа', 'сегодня', 'добрый', 'вопрос', 'место', 'друг', 'купить', 'делать', 'находить', 'проходить', 'становиться', 'спасибо', 'магазин', 'просто', 'работать', 'помогать', 'самый', 'школа', 'сделать', 'фото', 'ваш', 'говорить', 'думать', 'вообще', 'житель']\r\n",
      "\r\n",
      "Модель: 40 тем; 1 проходов по док-у\r\n",
      "Текст: прием глава город начинаться благоустройство улица чехов парковый фотография\r\n",
      "Токены по теме: ['сосед', 'район', 'дом', 'подсказывать', 'который', 'город', 'знать', 'день', 'новый', 'это', 'ребенок', 'привет', 'год', 'мочь', 'наш', 'ул', 'добрый', 'купить', 'улица', 'пожалуйста', 'свой', 'деньги', 'рубль', 'сделать', 'центр', 'житель', 'ремонт', 'первый', 'весь', 'дело', 'магазин', 'вопрос', 'сегодня', 'детский', 'проходить', 'собака', 'москва', 'время', 'каждый', 'россия', 'двор', 'цена', 'час', 'искать', 'квартира', 'здравствовать', 'телефон', 'находить', 'парк', 'большой']\r\n",
      "\r\n",
      "Модель: 10 тем; 1 проходов по док-у; -0.05 phi reg; -0.05 theta reg\r\n",
      "Текст: подсказывать обращаться компенсация случай дтп причина качество дорожный покрытие опыт обращение знать регион прецедент интересовать насколько это практиковаться\r\n",
      "Токены по теме: ['это', 'год', 'сосед', 'человек', 'который', 'наш', 'дом', 'район', 'день', 'знать', 'город', 'ребенок', 'подсказывать', 'свой', 'мочь', 'очень', 'новый', 'весь', 'хотеть', 'улица', 'добрый', 'хороший', 'время', 'работа', 'привет', 'место', 'становиться', 'просто', 'сделать', 'делать', 'пожалуйста', 'вопрос', 'собака', 'сегодня', 'квартира', 'думать', 'проходить', 'работать', 'почему', 'спасибо', 'друг', 'вообще', 'дорога', 'находить', 'ваш', 'писать', 'нужный', 'житель', 'вс', 'москва']\r\n",
      "\r\n",
      "Модель: 10 тем; 1 проходов по док-у; -10.00 phi reg; -10.00 theta reg\r\n",
      "Текст: таракан неделя кухня стоить травить обращаться быстро район сделать приходиться надолго уезжатть\r\n",
      "Токены по теме: ['это', 'год', 'человек', 'который', 'сосед', 'дом', 'наш', 'день', 'город', 'район', 'знать', 'мочь', 'ребенок', 'свой', 'весь', 'новый', 'очень', 'хотеть', 'время', 'улица', 'просто', 'работа', 'подсказывать', 'место', 'вопрос', 'сегодня', 'добрый', 'делать', 'собака', 'хороший', 'сделать', 'становиться', 'почему', 'работать', 'проходить', 'пожалуйста', 'привет', 'квартира', 'думать', 'вообще', 'ваш', 'сказать', 'друг', 'вс', 'магазин', 'помогать', 'житель', 'спасибо', 'ул', 'дорога']\r\n",
      "\r\n",
      "Модель: 40 тем; 1 проходов по док-у; -0.05 phi reg; -0.05 theta reg\r\n",
      "Текст: выставка мир женщина владимирский экспоцентр отличный возможность порадовать любимый половина осчастливливать покупка экспонент регион россия подарок находиться любой вкус кошелек\r\n",
      "Токены по теме: ['это', 'год', 'человек', 'сосед', 'день', 'знать', 'район', 'дом', 'который', 'город', 'наш', 'свой', 'мочь', 'подсказывать', 'хотеть', 'ребенок', 'весь', 'новый', 'улица', 'хороший', 'работа', 'добрый', 'очень', 'время', 'вопрос', 'привет', 'место', 'сегодня', 'работать', 'спасибо', 'пожалуйста', 'дорога', 'почему', 'просто', 'магазин', 'делать', 'становиться', 'житель', 'думать', 'квартира', 'проходить', 'вообще', 'помогать', 'купить', 'ваш', 'первый', 'сделать', 'давать', 'собака', 'обращаться']\r\n",
      "\r\n",
      "Модель: 40 тем; 1 проходов по док-у; -10.00 phi reg; -10.00 theta reg\r\n",
      "Текст: жизнь музей музейисториитомск мит воскресенскаягора томск историятомск нескучныймузей семейныевыходной детивмузей музейдеть новыйгоды новыйгодвтомск дизайнер художник декоратор выставка экскурсия интересно кудапойтивтомск детитомск мамочкитомск маматомск кудапойтисреб нкомтомск кудапойтисдетьмитомск отдыхсдеть\r\n",
      "Токены по теме: ['это', 'год', 'человек', 'который', 'сосед', 'дом', 'день', 'наш', 'свой', 'город', 'район', 'ребенок', 'знать', 'мочь', 'весь', 'хотеть', 'время', 'очень', 'работа', 'улица', 'новый', 'место', 'просто', 'добрый', 'сегодня', 'собака', 'подсказывать', 'вопрос', 'делать', 'работать', 'сделать', 'проходить', 'думать', 'хороший', 'сказать', 'вообще', 'становиться', 'привет', 'друг', 'квартира', 'житель', 'жить', 'говорить', 'дорога', 'почему', 'пожалуйста', 'начинать', 'ул', 'школа', 'ваш']\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cat examples.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1znaXEzi_SFR"
   },
   "outputs": [],
   "source": [
    "predict = best_model.transform(batch_vectorizer=batch_vectorizer)\n",
    "predict_t = predict.T # theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1znaXEzi_SFR"
   },
   "outputs": [],
   "source": [
    "predict_t.index.size, len(predict_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "007aq53d_SEL"
   },
   "outputs": [],
   "source": [
    "predict_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random.randint(0, high=len(predict_t), size=20)\n",
    "print(r)\n",
    "print(predict_t.iloc[r].values.argmax(axis=1))\n",
    "print(dict(predict_t.iloc[r].idxmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBhmUoLh_SFf"
   },
   "outputs": [],
   "source": [
    "def infer(model=best_model, n_examples=25):\n",
    "    r = np.random.randint(0, high=len(predict_t), size=n_examples)\n",
    "    # get the most probable topic for a processed text\n",
    "    # https://thispointer.com/pandas-find-maximum-values-position-in-columns-or-rows-of-a-dataframe/\n",
    "    samples = dict(predict_t.iloc[r].idxmax(axis=1))\n",
    "    tokens = model.score_tracker[\"top_words\"].last_tokens\n",
    "    for k, v in samples.items():\n",
    "        print()\n",
    "        try:\n",
    "            print(f\"Текст: {procced[k]}\\nТокены по теме: {tokens[v]}\")\n",
    "            print()\n",
    "        except KeyError:\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MCLqOvj8_SFi"
   },
   "outputs": [],
   "source": [
    "infer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FY5cY2-A_SFt"
   },
   "outputs": [],
   "source": [
    "infer(best_model_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TwVO416D_SFx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [
    "J97UzJGc_SFP"
   ],
   "name": "Continue.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
